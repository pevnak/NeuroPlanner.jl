var documenterSearchIndex = {"docs":
[{"location":"knowledge_base/#Knowledge-Base","page":"-","title":"Knowledge Base","text":"","category":"section"},{"location":"knowledge_base/","page":"-","title":"-","text":"The knowledge base is a structure, which (partially) expresses the computational graph. It is build on top of Mill.jl adding a missing feature to point to the array containing the data (Mill.jl strictly supports only computational trees). This enables the KnowledgeBase to reuse computation (the computational graph is directed acyclic graph), which is useful for expressing operation over graphs and hyper/multi graphs.","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"Before going into details, let start with a simple example implementing a simple graph neural network.","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"Let's assume the graph has 5 vertices, each desribed by 3 features","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"using NeuroPlanner\nusing NeuroPlanner.Mill\nusing NeuroPlanner.Flux\nx = randn(Float32, 3, 5)","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"Now, let's the graph has 4 edges [(1,2), (2,3), (3,4),(4,1)]. The simple graph neural network will first take features of corresponding vertices, concatenates them (vcat), and then it will aggregate them to each node. ","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"Concatenation of information about the edges is done using ProductNode from Mill.jl and it will concatenate the source and destination of each edge. ","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"xv = ProductNode((KBEntry(:x, [1,2,3,4]), KBEntry(:x, [2,3,4,1])))","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"Notice the use of KBEntry, which is a way how to refer to the data in the knowledge base (not yet constructed).","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"Then, each node needs to aggregate information about each edge. This is achieved by BagNode (again from Mill.jl), which identifies from which columns of xv each vertex receives information. In our case, the first vertex receives information from columns (1,4), the second from (2,3) etc. Effectively, the vertex receives information from each edge it contributes to. Therefore a simple message pass can be defined as ","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"BagNode(\n\tProductNode((\n\t\tKBEntry(:x, [1,2,3,4]), \n\t\tKBEntry(:x, [2,3,4,1]))), \n\tScatteredBags([[1,4],[1,2],[2,3],[3,4],Int[]])\n\t\t)","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"Finally, we can define the structured of the sample by wrapping to the KnowledgeBase as follows","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"ds = KnowledgeBase((;\n\tx = x,\n\tgnn_1 = BagNode(\n\tProductNode((\n\t\tKBEntry(:x, [1,2,3,4]), \n\t\tKBEntry(:x, [2,3,4,1]))), \n\t\tScatteredBags([[1,4],[1,2],[2,3],[3,4],Int[]]))\n\t))","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"The KnowledgeBase defines the structure of the sample. A corresponding KnowledgeModel can be defined manually, but we overload refletinmodel from Mill.jl for a convenience. The model can be defined as ","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"model = reflectinmodel(ds, d -> Dense(d, 16, relu);fsm = Dict(\"\" =>  d -> Dense(d, 1)))","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"and we can project the KnowledgeBase by model by model(ds), since model behaves like any other Flux layer.","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"julia> model(ds)\n1×5 Matrix{Float32}:\n 0.0850099  0.0129541  0.127564  0.202375  0.0","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"The construction of graphs can be simplified by EdgeBuilder.","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"While the above construction might seem clunky at first, it is relatively straightforward to create a hyper-graph. Let's again assume five vertices and hyper-edges [(1,2,3), (2,3,5), (5,4,5)]. This can be expressed by the following KnowledgeBase","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"ds = KnowledgeBase((;\n\tx = x,\n\tgnn_1 = BagNode(\n\tProductNode((\n\t\tKBEntry(:x, [1,2,5]), \n\t\tKBEntry(:x, [2,3,4]), \n\t\tKBEntry(:x, [3,5,5]))), \n\t\tScatteredBags([[1],[1,2],[1,2],[3],[2,3]]))\n\t))","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"We create a new model and we are ready to go","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"model = reflectinmodel(ds, d -> Dense(d, 16, relu);fsm = Dict(\"\" =>  d -> Dense(d, 1)))\nmodel(ds)","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"If you want to add more message passing layers, you just repeat the construction and replace the :x to refer to preceeding layer as ","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"ds = KnowledgeBase((;\n\tx = x,\n\tgnn_1 = BagNode(\n\t\tProductNode((\n\t\t\tKBEntry(:x, [1,2,3,4]), \n\t\t\tKBEntry(:x, [2,3,4,1]))), \n\t\t\tScatteredBags([[1,4],[1,2],[2,3],[3,4],Int[]])\n\t\t),\n\tgnn_2 = BagNode(\n\t\tProductNode((\n\t\t\tKBEntry(:gnn_1, [1,2,3,4]), \n\t\t\tKBEntry(:gnn_1, [2,3,4,1]))), \n\t\t\tScatteredBags([[1,4],[1,2],[2,3],[3,4],Int[]])\n\t\t)\n\t))","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"We create a new model and we are ready to go","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"model = reflectinmodel(ds, d -> Dense(d, 16, relu);fsm = Dict(\"\" =>  d -> Dense(d, 1)))\nmodel(ds)","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"Since adding message-passing layers over the same graph is overhead, there is KBEntryRenamer which facilitates this. The construction of the above can be simplified to ","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"using NeuroPlanner: KBEntryRenamer\n\nmplayer = BagNode(\n\t\tProductNode((\n\t\t\tKBEntry(:x, [1,2,3,4]), \n\t\t\tKBEntry(:x, [2,3,4,1]))), \n\t\t\tScatteredBags([[1,4],[1,2],[2,3],[3,4],Int[]])\n\t\t)\n\nds = KnowledgeBase((;\n\tx = x,\n\tgnn_1 = mplayer,\n\tgnn_2 = KBEntryRenamer(:x1, :gnn_2)(mplayer),\n\t))","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"Finally, we can create a multigraph mixing edges with hyperedges as","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"mplayer = ProductNode((;\n\t\t\tedges = BagNode(\n\t\t\t\tProductNode((\n\t\t\t\t\tKBEntry(:x, [1,2,3,4]), \n\t\t\t\t\tKBEntry(:x, [2,3,4,1]))), \n\t\t\t\t\tScatteredBags([[1,4],[1,2],[2,3],[3,4],Int[]])\n\t\t\t\t),\n\t\t\thyperedges = BagNode(\n\t\t\t\tProductNode((\n\t\t\t\t\tKBEntry(:x, [1,2,3,4]), \n\t\t\t\t\tKBEntry(:x, [2,3,4,1]))), \n\t\t\t\t\tScatteredBags([[1,4],[1,2],[2,3],[3,4],Int[]])\n\t\t\t\t)\n\t\t\t))\n\nds = KnowledgeBase((;\n\tx = x,\n\tgnn_1 = mplayer,\n\tgnn_2 = KBEntryRenamer(:x1, :gnn_2)(mplayer),\n\t))","category":"page"},{"location":"knowledge_base/","page":"-","title":"-","text":"The appropriate model can be created by reflectinmodel.","category":"page"},{"location":"knowledge_base/#Remarks","page":"-","title":"Remarks","text":"","category":"section"},{"location":"knowledge_base/","page":"-","title":"-","text":"KnowledgeBase behaves like samples, therefore numobs,  Mill.catobs, and batch are overloaded for minibatching. Though we do not implement MLUtils.getobs, since that would be quite complicated.\ndeduplicate remove duplicities from the knowledgebase without intacting the output.","category":"page"},{"location":"usage_guide/#Usage-guide","page":"Usage guide","title":"Usage guide","text":"","category":"section"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"The Neuroplanner library allows for creation and training of a heuristic function to use in planning problems. This function is represented by a neural network model; therfore, for it to be ready for use, several steps must be completed.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"","category":"page"},{"location":"usage_guide/#Model-creation","page":"Usage guide","title":"Model creation","text":"","category":"section"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"To begin, necessarry libraries are imported.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"using NeuroPlanner\nusing NeuroPlanner.PDDL\nusing NeuroPlanner.Flux\nusing NeuroPlanner.SymbolicPlanners\nusing PDDL: GenericProblem\nusing NeuroPlanner.SymbolicPlanners: PathSearchSolution\nusing Statistics\nusing Random\nusing Accessors\nusing NeuroPlanner.Mill","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"Then a representation of the domain we will be working with is needed. PDDL has a helper function load_domain() into which the string path locationof domain.pddl file to be loaded is passed. ","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"domain = load_domain(\"../domains/ferry/domain.pddl\")","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"Is also helpful to keep the string paths to the problem files we will be dealing with. They can be loaded for example as such:","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"problem_files = [joinpath(\"../domains/ferry/\", f) for f in readdir(\"../domains/ferry\") if endswith(f,\".pddl\") && f !== \"domain.pddl\"]","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"Problems from the domain require an extractor (here called pddld) to be parsed in a standardised form. When creating an extractor we can choose from diferrent architectures (ASNet, HyperExtractor, HGNNLite, HGNN, LevinASNet), which fundamentally change how the problem, and as a result the model, are internally represented and will affect how the final heuristic behaves. Details here: Extractors, Theoretical background.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"pddld = ASNet(domain)\npddld = HGNNLite(domain)\npddld = HGNN(domain)\npddld = ObjectBinary(domain)\npddld = AtomBinary(domain)\npddld = ObjectAtom(domain)\npddld = ObjectAtomBip(domain)\npddld = LevinASNet(domain)","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"Note that the last LevinASNet is a version of ASNet adapted to be used with the LevinLoss and LevinAStar to implement the method from the paper","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"Now the chosen extractor has to be specialized for a given problem instance. First a problem is loaded with the load_problem() function (any problem from the set will do, we are choosing the first one for simplicity). Next specialize() is used to create the specialized extractor pddle, and create the initial state of the problem with initstate(domain, problem)","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"problem = load_problem(first(problem_files))\npddle = specialize(pddld, problem)\nstate = initstate(domain, problem)","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"Alternatively both specialization of the extractor and creation of the inital state can be done by calling initproblem()","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"pddle, state = initproblem(pddld, problem)","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"To use the specialized extractor, its functor is called on the state to be extracted. The inital state is extracted so the model can be created from it later.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"h₀ = pddle(state)","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"With the extracted state the model is created with the Mill.jl reflectinmodel() function. This is an example of how the model creation can look. Brief explanation of the args used here:","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"h₀ A state that the model will be able to process, specifies structure of the model.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"d -> Dense(d, dense_dim, relu) Setting of dense layers, output will be = dense_dim, activation function is relu.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"fsm = Dict(\"\" =>  d -> ffnn(d, dense_dim, 1, dense_layers)) optional kwarg, overrides constructions of feed-forward models. Here \"\" in fsm denotes the last (output) layer, so these are settings for the last layer. (The 1 is the output dimension)","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"model = reflectinmodel(h₀, d -> Dense(d, 32, relu);fsm = Dict(\"\" =>  d -> Chain(Dense(d,32,relu), Dense(32,1))))","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"For more optional keyword arguments and info on this function see Mill.jl documentation on the topic.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"","category":"page"},{"location":"usage_guide/#Model-training","page":"Usage guide","title":"Model training","text":"","category":"section"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"To make training efficient the data is split into minibatches. In NeuroPlanner minibatches are tied one-to-one with loss functions; each loss function has a minibatch constructor that prepares the data in a way that the loss function can parse. Options for loss functions are: l2, l₂, lstar, lₛ, lgbfs, lrt, bellman, levinloss. With any of these the constructor can be created. For info on differences in loss functions see Losses, or the Theoretical background.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"fminibatch = NeuroPlanner.minibatchconstructor(\"l2\") \nfminibatch = NeuroPlanner.minibatchconstructor(\"l₂\") \nfminibatch = NeuroPlanner.minibatchconstructor(\"lstar\") \nfminibatch = NeuroPlanner.minibatchconstructor(\"lₛ\") \nfminibatch = NeuroPlanner.minibatchconstructor(\"lgbfs\") \nfminibatch = NeuroPlanner.minibatchconstructor(\"lrt\") \nfminibatch = NeuroPlanner.minibatchconstructor(\"bellman\") \nfminibatch = NeuroPlanner.minibatchconstructor(\"levinloss\") ","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"With the constructor made, the batches can be created with the code snippet below. Deduplicate is a Neuroplanner function which eliminates redudant data from the batches, cutting down the size dramatically.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"train_files = filter(isfile ∘ NeuroPlanner.plan_file, problem_files)\nminibatches = map(train_files) do problem_file\n\t\t\t  plan = load_plan(problem_file)\n\t\t \t  problem = load_problem(problem_file)\n\t\t\t  ds = fminibatch(pddld, domain, problem, plan)\n\t\t\t  dedu = @set ds.x = deduplicate(ds.x)\n\t\t\t  end","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"An optimiser is chosen (any works, for brevity AdaBelief() is picked here) and model parameters are extracted.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"using Flux.Optimisers\nopt_state = Optimisers.setup(Optimisers.AdaBelief(), model) ","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"The model is trained with train!(). ","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"NeuroPlanner.loss passes the generic NeuroPlanner loss, which will dispatch to the correct loss based on the minibatch passed to it.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"model,opt_state are the pre-prepared structs.  ","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"for i in 1:10_000\n\tmb = rand(minibatches)\n\tl, ∇model = Flux.withgradient(model -> NeuroPlanner.loss(model, mb), model)\n\tstate_tree, model = Optimisers.update(state_tree, model, ∇model[1]);\nend","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"With this the model is trained and ready to be used.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"","category":"page"},{"location":"usage_guide/#Model-usage","page":"Usage guide","title":"Model usage","text":"","category":"section"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"The simplest way of using the trained model is to call its functor. It computes the value of the heuristic function for the extracted state passed to it.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"heur_value = model(pddle(state))","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"This, however, can be slow and unwieldy to work with at a large scale. NeuroPlanner provides several helper functions to make working with a trained model easier.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"solve_problem() solves the problem in the problem_files, using the passed model, extractor and planner. It returns a solution object, which has fields describing details of how the problem was solved.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"sol = solve_problem(pddld, problem_file, model, planner; return_unsolved = true)","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"If we want to compare to a non-heuristic solution, the solveproblem() solves the problem using a standard forward planner with a null heuristic. It only takes the domain of the problem and the problem file`` as the input.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"sol = solveproblem(domain, problem_file)","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"It is also an option to create a NeuroHeuristic() which behaves like a normal heuristic object and can be passed to constructors of planners.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"hfun = NeuroHeuristic(pddld, problem, model)\nplanner = AStarPlanner(hfun)","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"The planners availiable are [AStarPlanner, GreedyPlanner, W15AStarPlanner, W20AStarPlanner] and BFSPlanner which is used with the LevinAsnet model. Once created, the planner can be used to also get the solution of a given problem.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"sol = planner(domain, state₀, PDDL.get_goal(problem))","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"To quickly display a collection of solutions, the show_stats which prints relevant data to console.","category":"page"},{"location":"usage_guide/","page":"Usage guide","title":"Usage guide","text":"show_stats(solutions)","category":"page"},{"location":"theory/#Theorectial-background","page":"Theoretical background","title":"Theorectial background","text":"","category":"section"},{"location":"theory/","page":"Theoretical background","title":"Theoretical background","text":"","category":"page"},{"location":"theory/#F.A.Q.","page":"Theoretical background","title":"F.A.Q.","text":"","category":"section"},{"location":"extractors/#Extractors","page":"Extractors","title":"Extractors","text":"","category":"section"},{"location":"extractors/","page":"Extractors","title":"Extractors","text":"ObjectAtom","category":"page"},{"location":"extractors/#NeuroPlanner.ObjectAtom","page":"Extractors","title":"NeuroPlanner.ObjectAtom","text":"struct ObjectAtom{DO,D,N,MP,S,G}\n    domain::DO\n    multiarg_predicates::NTuple{N,Symbol}\n    nunanary_predicates::Dict{Symbol,Int64}\n    objtype2id::Dict{Symbol,Int64}\n    constmap::Dict{Symbol,Int64}\n    model_params::MP\n    obj2id::D\n    init_state::S\n    goal_state::G\nend\n\nRepresents STRIPS state as a hypergraph, where \n\nvertices corresponds to object and constants (map is stored in obj2id)\nunary atoms (predicates) are propeties of its arguments (map of type of the atom to the feature id is the position in multiarg_predicates)\nnullary atoms (predicate) are properties of all objects\nn-ary atoms (predicates) are represented as hyper-edges over its arguments (objects)\n\nThe computational model is message-passing over hyper-graph, which is essentially  a message passing over a bipartite graph, where left vertices corresponds to vertices in hypergraph and right vertices corresponds to hyper-edges. There is an edge between vertex corresponding to the hyper-edge and its vertices.\n\nFollowing parameters are initialized for domain when ObjectAtom(domain) is called\n\nmultiarg_predicates is a list of all n-ary predicates\nnunanary_predicates maps unary predicates to an index in one-hot encoded vertex' properties \nobjtype2id maps unary predicates to an index in one-hot encoded vertex' properties \nconstmap maps constants to an index in one-hot encoded vertex' properties \nmodel_params some parameters of an algorithm constructing the message passing passes \n\nFollowing parameters are initiliazed when extractor is specialized per problem instance\n\nobj2id contains a map from object names to their ids. This is created when calling specialize(ex::ObjectAtom, problem)\ninit_state is information about initial state. If nothing it is added to the represenation of the state allowing a    to measure heuristic distance between initial_state and given state\ninit_state is information about goal state. If nothing it is added to the represenation of the state allowing a    to measure heuristic distance between goal_state and given state\n\nConstructors:\n\nObjectAtom(domain; message_passes=2, residual=:linear, kwargs...) The basic constructor \n\nspecializing for a given domain. Notice the arguments message_passes and  residual specifying the number of layers and the use of residual connections.\n\nObjectAtom(domain, problem; embed_goal=true, kwargs...) specializes the constructor for a \n\ngiven domain and problem instance. The arguments are message_passes, residual, and additional embed_goal which will automatically extract information about the goal state.\n\n\n\n\n\n","category":"type"},{"location":"extractors/","page":"Extractors","title":"Extractors","text":"ObjectAtomBip","category":"page"},{"location":"extractors/#NeuroPlanner.ObjectAtomBip","page":"Extractors","title":"NeuroPlanner.ObjectAtomBip","text":"struct ObjectAtomBip{DO,P,EB,MP,D,II,S,G}     domain::DO     predicates::P     edgebuilder::EB     objtype2id::Dict{Symbol,Int64}     constmap::Dict{Symbol,Int64}     modelparams::MP     obj2id::D     cachedtypes::II     initstate::S     goalstate::G end\n\nRepresents STRIPS state as a bipartite graph, where vertices corresponds to object,  constants (map is stored in obj2id), and atoms. There is an edge between object  and atom, if the atom has the object as an argument.\n\nFollowing parameters are initialized for domain when ObjectAtomBip(domain) is called\n\npredicates contains information about predicates as produced by PredicateInfo(domain).    It contains list of predicates, their arrities, number of predicates with arity higher than two,   and number of predicates with arity 0 and 1. Finally, it contains a map of predicate names to    indices, which is used in intstates to convert names to ids\nobjtype2id maps object types to an index in one-hot encoded vertex' properties \nconstmap maps constants to an index in one-hot encoded vertex' properties \nmodel_params some parameters of an algorithm constructing the message passing passes \nedgebuilder is used to specify, if different types of edges are represented as edges \n\nwith features (use FeaturedEdgeBuilder) or use multigraph with edges of multiple types (use MultiEdgeBuilder).\n\nFollowing parameters are initiliazed when extractor is specialized per problem instance\n\nobj2id contains a map from object names to their ids. This is created when calling specialize(ex::ObjectAtom, problem)\ninit_state is information about initial state. If nothing it is added to the represenation of the state allowing a    to measure heuristic distance between initial_state and given state\ninit_state is information about goal state. If nothing it is added to the represenation of the state allowing a    to measure heuristic distance between goal_state and given state\ncached_types contains a cache of (objectid, types), which makes the repeated extraction of states from the same\n\nproblem instance faster.\n\nConstructors:\n\nObjectAtomBip(domain; message_passes=2, residual=:linear, edgebuilder = FeaturedEdgeBuilder, kwargs...) The basic constructor \n\nspecializing for a given domain. Notice the arguments message_passes and  residual specifying the number of layers and the use of residual connections.  Furthermore, there is an edgebuilder specifying how edges will be represented.\n\nObjectAtomBip(domain, problem; embed_goal=true, kwargs...) specializes the constructor for a \n\ngiven domain and problem instance. The arguments are message_passes, residual, and additional embed_goal which will automatically extract information about the goal state.\n\nWe define three convenient shortcuts to different variants of representation of edges: \n\nObjectAtomBipFE represents different types of edges as edges with features. The edges \n\nare deduplicated, which means that if there are more edges, between two vertices, then they will be represented as a single edge with features aggregated (by default by max).\n\nObjectAtomBipFENA is similar as `ObjectAtomBipFE, but the edges are not deduplicated.\nObjectAtomBipME represents different edges as multi-graph.\n\n\n\n\n\n","category":"type"},{"location":"extractors/","page":"Extractors","title":"Extractors","text":"ObjectBinary","category":"page"},{"location":"extractors/#NeuroPlanner.ObjectBinary","page":"Extractors","title":"NeuroPlanner.ObjectBinary","text":"struct ObjectBinary{DO,P,EB,MP,D,II,S,G}     domain::DO     predicates::P     edgebuilder::EB     objtype2id::Dict{Symbol,Int64}     constmap::Dict{Symbol,Int64}     modelparams::MP     obj2id::D     cachedtypes::II     initstate::S     goalstate::G end\n\nRepresents STRIPS state as a graph, where vertices corresponds to objects and contants  constants (map is stored in obj2id). There is an edge between objects, if they are both in the same atom with arity higher than two. The type of the atom (predicate) is considered as a type / feature of the edge. Unary and nullary predicates are represented as feature  of the vertex / all vertices.\n\nFollowing parameters are initialized for domain when ObjectAtomBip(domain) is called\n\npredicates contains information about predicates as produced by PredicateInfo(domain).    It contains list of predicates, their arrities, number of predicates with arity higher than two,   and number of predicates with arity 0 and 1. Finally, it contains a map of predicate names to    indices, which is used in intstates to convert names to ids\nobjtype2id maps object types to an index in one-hot encoded vertex' properties \nconstmap maps constants to an index in one-hot encoded vertex' properties \nmodel_params some parameters of an algorithm constructing the message passing passes \nedgebuilder is used to specify, if different types of edges are represented as edges \n\nwith features (use FeaturedEdgeBuilder) or use multigraph with edges of multiple types (use MultiEdgeBuilder).\n\nFollowing parameters are initiliazed when extractor is specialized per problem instance\n\nobj2id contains a map from object names to their ids. This is created when calling specialize(ex::ObjectAtom, problem)\ninit_state is information about initial state. If nothing it is added to the represenation of the state allowing a    to measure heuristic distance between initial_state and given state\ninit_state is information about goal state. If nothing it is added to the represenation of the state allowing a    to measure heuristic distance between goal_state and given state\ncached_types contains a cache of (objectid, types), which makes the repeated extraction of states from the same\n\nproblem instance faster.\n\nConstructors:\n\nObjectBinary(domain; message_passes=2, residual=:linear, edgebuilder = FeaturedEdgeBuilder, kwargs...) The basic constructor \n\nspecializing for a given domain. Notice the arguments message_passes and  residual specifying the number of layers and the use of residual connections.  Furthermore, there is an edgebuilder specifying how edges will be represented.\n\nObjectBinary(domain, problem; embed_goal=true, kwargs...) specializes the constructor for a \n\ngiven domain and problem instance. The arguments are message_passes, residual, and additional embed_goal which will automatically extract information about the goal state.\n\nWe define three convenient shortcuts to different variants of representation of edges: \n\nObjectBinaryFE represents different types of edges as edges with features. The edges \n\nare deduplicated, which means that if there are more edges, between two vertices, then they will be represented as a single edge with features aggregated (by default by max).\n\nObjectBinaryFENA is similar as `ObjectBinaryFE, but the edges are not deduplicated.\nObjectBinaryME represents different edges as multi-graph.\n\n\n\n\n\n","category":"type"},{"location":"extractors/","page":"Extractors","title":"Extractors","text":"AtomBinary","category":"page"},{"location":"extractors/#NeuroPlanner.AtomBinary","page":"Extractors","title":"NeuroPlanner.AtomBinary","text":"struct AtomBinary{DO,EB,MP,D,S,G}\n    domain::DO\n    edgebuilder::EB\n    max_arity::Int\n    constmap::Dict{Symbol,Int}\n    actionmap::Dict{Symbol,Int}\n    model_params::MP\n    obj2id::D\n    init_state::S           # contains atoms of the init state\n    goal_state::G           # contains atoms of the goal state\nend\n\nRepresents a PDDL state as a hypergraph, whre \n\nEach node is either an object or a contant\nunary predicate is a property of an object\nnullary predicate is a property of all objects\nn-ary predicate is a hyper-edge\n\nThe computational model is message-passing over hyper-graph, which is essentially  a message passing over a bipartite graph, where left vertices corresponds to vertices in hypergraph and right vertices corresponds to hyper-edges. There is an edge between vertex corresponding to the hyper-edge and its vertices.\n\n–- multiarg_predicates is a list of all n-ary predicates –- nunary_predicates maps unary predicates to an index in one-hot encoded vertex' properties  –- objtype2id maps unary predicates to an index in one-hot encoded vertex' properties  –- constmap maps constants to an index in one-hot encoded vertex' properties  –- model_params some parameters of an algorithm constructing the message passing passes \n\n\n\n\n\n","category":"type"},{"location":"#NeuroPlanner.jl","page":"Home","title":"NeuroPlanner.jl","text":"","category":"section"},{"location":"#Motivation","page":"Home","title":"Motivation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This library implements two things:","category":"page"},{"location":"","page":"Home","title":"Home","text":"heuristic functions for STRIPS domains implemented by (hyper-multi-) graph neural networks; \nloss functions to optimize parameters of the heuristic functions from a solved plans.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The library is experimental in the sense that it is used to research different phenomenons and trade-offs.  This also means that things can change.","category":"page"},{"location":"","page":"Home","title":"Home","text":"An api of heuristic function api and few details and gotchas can be found here.","category":"page"},{"location":"#Short-snipper","page":"Home","title":"Short snipper","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using NeuroPlanner\nusing NeuroPlanner.PDDL\nusing NeuroPlanner.Mill\nusing NeuroPlanner.Flux\nusing NeuroPlanner.SymbolicPlanners\nusing NeuroPlanner.Flux.Optimisers\ndomain = load_domain(\"../domains/ferry/domain.pddl\")\nproblem_files = [joinpath(\"../domains/ferry/\", f) for f in readdir(\"../domains/ferry\") if endswith(f,\".pddl\") && f !== \"domain.pddl\"]\ntrain_files = filter(s -> isfile(plan_file(s)), problem_files)\nproblem = load_problem(first(problem_files))\n\npddld = ObjectAtom(domain)\npddle, state = initproblem(pddld, problem)\nh₀ = pddle(state)\nmodel = reflectinmodel(h₀, d -> Dense(d, 32, relu);fsm = Dict(\"\" =>  d -> Chain(Dense(d, 32, relu), Dense(32,1))))\n\nfminibatch = NeuroPlanner.minibatchconstructor(\"lstar\") \nminibatches = map(train_files) do problem_file\n\t\t\tplan = NeuroPlanner.load_plan(problem_file)\n\t\t\tproblem = load_problem(problem_file)\n\t\t\tds = fminibatch(pddld, domain, problem, plan)\n\t\tend\n\nstate_tree = Optimisers.setup(Optimisers.AdaBelief(), model) \nfor i in 1:10_000\n\tmb = rand(minibatches)\n\tl, ∇model = Flux.withgradient(model -> NeuroPlanner.loss(model, mb), model)\n\tstate_tree, model = Optimisers.update(state_tree, model, ∇model[1]);\nend","category":"page"},{"location":"#A-more-detailed-walkthrough","page":"Home","title":"A more detailed walkthrough","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A naive example of learning a heuristic on a domain","category":"page"},{"location":"","page":"Home","title":"Home","text":"First we load the domain and problem files","category":"page"},{"location":"","page":"Home","title":"Home","text":"using NeuroPlanner\nusing NeuroPlanner.PDDL\ndomain = load_domain(\"../domains/ferry/domain.pddl\")\nproblem_files = [joinpath(\"../domains/ferry/\", f) for f in readdir(\"../domains/ferry\") if endswith(f,\".pddl\") && f !== \"domain.pddl\"]\ntrain_files = filter(s -> isfile(plan_file(s)), problem_files)\nproblem = load_problem(first(problem_files))","category":"page"},{"location":"","page":"Home","title":"Home","text":"Then we create an extractor pddld for the chosen domain \"ferry\", here we will pick the ObjectAtom extractor. The role of the extracor is to convert the STRIPS state to a representation suitable for the neural networks. The specialization for the domain fixes information which are constant ","category":"page"},{"location":"","page":"Home","title":"Home","text":"pddld = ObjectAtom(domain)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The extractor is then specialised for the given problem, this is done by the initproblem() function which also return the initial state of the problem. With the specialised extractor (pddle) we can then create the extracted inital state h₀.  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"pddle, state = initproblem(pddld, problem)\nh₀ = pddle(state)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Now we can create a model to represent the heuristic. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using NeuroPlanner.Mill\nusing NeuroPlanner.Flux\nusing NeuroPlanner.SymbolicPlanners\nmodel = reflectinmodel(h₀, d -> Dense(d, 32, relu);fsm = Dict(\"\" =>  d -> Chain(Dense(d, 32, relu), Dense(32,1))))","category":"page"},{"location":"","page":"Home","title":"Home","text":"To allow for efficient training, we create minibatches from the training files. Each loss function has its own type of minibatch, which allows a dispatch of the loss on the type of the minibatch. In below example, the function fminibatch therefore creates minibatch for loss function named \"lstar\", which is designed to maximize efficiency of A* algorithm [1].","category":"page"},{"location":"","page":"Home","title":"Home","text":"fminibatch = NeuroPlanner.minibatchconstructor(\"lstar\") \nminibatches = map(train_files) do problem_file\n\t\t\tplan = NeuroPlanner.load_plan(problem_file)\n\t\t\tproblem = load_problem(problem_file)\n\t\t\tds = fminibatch(pddld, domain, problem, plan)\n\t\tend","category":"page"},{"location":"","page":"Home","title":"Home","text":"With minibatches, we train the model using standart training loop.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using NeuroPlanner.Flux.Optimisers\nstate_tree = Optimisers.setup(Optimisers.AdaBelief(), model) \nfor i in 1:10_000\n\tmb = rand(minibatches)\n\tl, ∇model = Flux.withgradient(model -> NeuroPlanner.loss(model, mb), model)\n\tstate_tree, model = Optimisers.update(state_tree, model, ∇model[1]);\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finally, we evaluate the model inside the A* algorith,=m.","category":"page"},{"location":"","page":"Home","title":"Home","text":"map(s -> NeuroPlanner.solve_problem(pddld, load_problem(s), model, AStarPlanner), setdiff(problem_files, train_files))","category":"page"},{"location":"","page":"Home","title":"Home","text":"initproblem(ex, problem; add_goal = true)","category":"page"},{"location":"#NeuroPlanner.initproblem-Tuple{Any, Any}","page":"Home","title":"NeuroPlanner.initproblem","text":"initproblem(ex, problem; add_goal = true)\n\nSpecialize extractor for the given problem instance and return init state \n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"[1]: Chrestien, Leah, et al. \"Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal.\" Advances in Neural Information Processing Systems 36 (2024).","category":"page"},{"location":"heuristic/#Heuristic-functions","page":"Heuristic","title":"Heuristic functions","text":"","category":"section"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"The heuristic function is composed of two parts. First, called extractor, ex takes a STRIPS state s and project it to to (computation) graph (more on this later). The second is the neural network nn, which takes the computation graph and project it to the heuristic function. This functionality is based on a custom extension of Mill.jl library.  The complete heuristic function is therefore composition nn ∘ ex. ","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"The advantage is that non-differentiable operations are kept in the the extractor part and differentiable in the neural network. The main advantage of the extractor producing the computational graph is that it can be then deduplicated, which speeds-up the traingn.","category":"page"},{"location":"heuristic/#Extractors","page":"Heuristic","title":"Extractors","text":"","category":"section"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"The library implements various methods how to represent STRIPS states as graphs. This representation is important for the properties of the heuristic function, mainly to its ability to discriminate between states. This representation is perpendicular to the type of graph neural networks, in which this library in not that interested that much. The functionality projecting state to graph is called extractor.","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"The available extractors are:","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"ObjectAtom[1] represent state as a hyper-multi-graph. Each vertex corresponds to the object, each atom represent a hyper-edge. Different types of atoms are represented as different type of edges.\nObjectAtomBip[1] represent state as a multi-graph or graphs with features on edges. Each object and atom corresponds to a vertex. Object-vertex is connected to atom-vertex when object is an argument of the atom. The representation is similar to the ObjectAtom, except the hyper-edges are represented in bipartite graph.\nObjectBinary[1] represent states as multi graph (or graph with features on edges). Each object correponds to the vertex. Vertices are connected by the edge if they are in the same atom. The type of edge (or features one edges) corresponds to the type of atom and position of the object in the argument.\nAtomBinary[1] represent states as multi graph (or graph with features on edges). Each object correponds to the atom. Vertices are connected by the edge if they share the same object. The type of the edge (or features one edges) corresponds to the position of the object in both atoms.\nObjectPair[1] each vertex corresponds to a tuple of objects and edges are create by some cryptic algorithm.\nASNet[2] creates vertices for each possible atoms. The atoms are present in the graph even when they are not true in the state. This means that graph representing states differ only in features on edges, which codes if the atom is true or false. \nHGNN[3] is similar to ASNet, except the message-passing over the hyper-edges is a bit different, as it includes more domain knowledge from the planning community.","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"Let's now focus on extraction function ex, which takes a state s and converts it to some representation suitable for NN. In the case of this library, the representation is an instance of KnowledgeBase, which encodes the copmutation graph. Since the extractor produces the compuation graph, the extraction function controls the number of graph convolutions and the presence of residual connections. ","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"The extraction function is implemented as a callable struct with a following api, where ObjectBinary is used as an example: The api / interface of the extraction function is as follows:","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"ex = ObjectBinary(domain; message_passes = 2, residual=:linear, edgebuilder = FeaturedEdgeBuilder) –- initialize the extractor for a given domain. At this moment, we need to specify the number of message passes and the type of residula layer (:none or :linear). Additionally, you specify how to represent edges pf different types by passing the edgebuilder. The default  FeaturedEdgeBuilder, uses is edges with features, other option is MultiEdgeBuilder  which uses multi-graph (multiple)\nex = specialize(ex, problem)  –- specialize the extractor functions for a given domain\nex = add_goalstate(ex, problem, goal = goalstate(domain, problem) –- fixes a goal state in the extractor\nex = add_initstate(ex, problem, start = initstate(domain, problem) –- fixes an initial state in the extractor.","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"With this, ex(state) converts the state to a structure for the neural network, an instance of KnowledgeBase in this concerete example.","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"All extraction functions has to be initialized for a given domain and specialized for a given problem. This is typically needed to initiate various mapping to ensure it does not change between problem instances (an example is a map of categorical variables to one-hot representations).  Adding goal or init state is optional. If they are added, the input to the neural network would always contain goal or init state, in which case the neural network will measure a distance to a state. If they are not used, the neural network can be used to create and embedding of states. ","category":"page"},{"location":"heuristic/#Example","page":"Heuristic","title":"Example","text":"","category":"section"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"Load the libraries, domain, and problem","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"using NeuroPlanner\nusing NeuroPlanner.PDDL\nusing NeuroPlanner.Flux\nusing NeuroPlanner.Mill\nusing NeuroPlanner.SymbolicPlanners\nusing PlanningDomains\n\ndomain = load_domain(IPCInstancesRepo,\"ipc-2014\", \"barman-sequential-satisficing\")\nproblems = list_problems(IPCInstancesRepo, \"ipc-2014\", \"barman-sequential-satisficing\")\nproblem = load_problem(IPCInstancesRepo, \"ipc-2014\", \"barman-sequential-satisficing\", first(problems))","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"First, we create the ObjectBinary for the domain","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"julia> ex = ObjectBinary(domain)\nUnspecialized extractor for barman (6, 9)","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"Then, we specialize the extractor for a problem","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"julia> specex = specialize(ex, problem)\nSpecialized extractor without goal for barman (6, 9, 40)","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"If we would like to use the extractor to measure a distance to the goal, we add the goal","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"julia> specex = add_goalstate(specex, problem,)\nSpecialized extractor with goal for barman (6, 9, 40)","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"The function add_goalstate has a third implicit parameter goal =  goalstate(domain, problem), which allows to specify different goal then the default for the problem. Also, the function checks, if the extraction function is specialized for the problem and if not, it specialize it. Hence, the above can be shorted as ","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"julia> specex = add_goalstate(ObjectAtom(domain), problem)\nSpecialized extractor with goal for barman (6, 9, 40)","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"With specialized extraction function, we can convert a state to a KnowledgeBase as ","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"julia> s = initstate(domain, problem);\njulia> specex(s)\nKnowledgeBase: (x1,gnn_2,res_3,gnn_4,res_5,o)","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"The neural network processing the KnowledgeBase can be initialized as the neural network in Mill.jl library through extended reflectinmodel function","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"julia> kb = specex(s);\njulia> model = reflectinmodel(specex(s), d -> Dense(d,10), SegmentedMean;fsm = Dict(\"\" =>  d -> Dense(d,1)))\nKnowledgeModel: (gnn_2,res_3,gnn_4,res_5,o)","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"which finishes the construction of the heuristic function as ","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"julia> model(specex(s))\n1×1 Matrix{Float32}:\n 0.003934524","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"The parameters of the model can be optimized using the standard method of Flux.jl on top of which they are built. We refer the reader to the documentation of Mill.jl for details of the reflectinmodel function.","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"A complete example look like:","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"using NeuroPlanner\nusing NeuroPlanner.PDDL\nusing NeuroPlanner.Flux\nusing NeuroPlanner.Mill\nusing NeuroPlanner.SymbolicPlanners\nusing PlanningDomains\n\ndomain = load_domain(IPCInstancesRepo,\"ipc-2014\", \"barman-sequential-satisficing\")\nproblems = list_problems(IPCInstancesRepo, \"ipc-2014\", \"barman-sequential-satisficing\")\nproblem = load_problem(IPCInstancesRepo, \"ipc-2014\", \"barman-sequential-satisficing\", first(problems))\n\nex = ObjectAtom(domain)\nspecex = specialize(ex, problem)\nspecex = add_goalstate(ex, problem, goalstate(domain, problem))\ns = initstate(domain, problem)\nmodel = reflectinmodel(specex(s), d -> Dense(d,10), SegmentedMean;fsm = Dict(\"\" =>  d -> Dense(d,1)))\n\nmodel(specex(s))","category":"page"},{"location":"heuristic/#Remarks","page":"Heuristic","title":"Remarks","text":"","category":"section"},{"location":"heuristic/#First-remark:-the-model-is-general","page":"Heuristic","title":"First remark: the model is general","text":"","category":"section"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"The model is able to process any problem instance despite it has been constructed from a state on a given problem instance. This can be seen on the following example which assumes the above model and uses the model on all problem instances from barman. Notice that the extractor needs to be specialized for every problem instance.","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"ex = ObjectAtom(domain)\n\nmap(problems) do problem_name\n\tproblem = load_problem(IPCInstancesRepo, \"ipc-2014\", \"barman-sequential-satisficing\", problem_name)\n\ts = initstate(domain, problem)\n\tspecex = add_goalstate(ex, problem)\n\tonly(model(specex(s)))\nend","category":"page"},{"location":"heuristic/#Second-remark:-fixing-initial-state-measures-distance-from-initial-state","page":"Heuristic","title":"Second remark: fixing initial state measures distance from initial state","text":"","category":"section"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"If the extractor is specialized with the goalstate, it meaures a distances from the state to a goalstate. On the other hand if the extractor is specialized with the init state, it will measure distance from init state to a state. Hence a distance from init to goal state can be computed by both specializations as is shown in the following example","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"ex = ObjectAtom(domain)\nproblem = load_problem(IPCInstancesRepo, \"ipc-2014\", \"barman-sequential-satisficing\", first(problems))\n\niex = add_initstate(ex, problem)\ngex = add_goalstate(ex, problem)\n\nsi = initstate(domain, problem)\ngi = goalstate(domain, problem)\nmodel = reflectinmodel(iex(si), d -> Dense(d,10), SegmentedMean;fsm = Dict(\"\" =>  d -> Dense(d,1)))\n\nmodel(iex(goalstate(domain, problem))) ≈ model(gex(initstate(domain, problem)))","category":"page"},{"location":"heuristic/#Third-remark:-extractor-without-goal-is-useful-for-creating-an-embedding","page":"Heuristic","title":"Third remark: extractor without goal is useful for creating an embedding","text":"","category":"section"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"May-be, we do not want a neural network to implement a heuristic function, but to project the state to a vector. This can be done with a specialized extractor without goal as ","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"problem = load_problem(IPCInstancesRepo, \"ipc-2014\", \"barman-sequential-satisficing\", first(problems))\nex = specialize(ObjectAtom(domain), problem)\n\nsi = initstate(domain, problem)\ngi = goalstate(domain, problem)\nmodel = reflectinmodel(ex(si), d -> Dense(d,10), SegmentedMean;fsm = Dict(\"\" =>  d -> Dense(d,3)))","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"now the model will project states to the 3-dimensional vector as","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"julia> model(ex(si))\n3×1 Matrix{Float32}:\n  0.048694983\n -0.35071477\n -0.013481511\n","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"Notice the difference in the argument fsm = Dict(\"\" =>  d -> Dense(d,3)) in the argument of the reflectinmodel.","category":"page"},{"location":"heuristic/#Fourth-remark:-extracted-states-can-be-batched","page":"Heuristic","title":"Fourth remark: extracted states can be batched","text":"","category":"section"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"KnowledgeBase supports Flux.batch for minibatching. Using the above example, we can create a minibatch containing initial and goal state as","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"julia> b = Flux.batch([ex(si), ex(gi)])\nKnowledgeBase: (x1,gnn_2,res_3,gnn_4,res_5,o)","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"and project it with the model","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"julia> model(b)\n3×2 Matrix{Float32}:\n  0.048695    0.061797\n -0.350715    0.111813\n -0.0134815  -0.0315447","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"[1]: Horčík, Rostislav, and Gustav Šír. \"Expressiveness of Graph Neural Networks in Planning Domains.\" Proceedings of the International Conference on Automated Planning and Scheduling. Vol. 34. 2024.","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"[2]: Toyer, Sam, et al. \"Action schema networks: Generalised policies with deep learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018.","category":"page"},{"location":"heuristic/","page":"Heuristic","title":"Heuristic","text":"[3]: Learning Domain-Independent Planning Heuristics with Hypergraph Networks, William Shen, Felipe Trevizan, Sylvie Thiebaux, 2020","category":"page"}]
}
