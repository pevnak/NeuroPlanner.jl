<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Heuristic · NeuroPlanner</title><meta name="title" content="Heuristic · NeuroPlanner"/><meta property="og:title" content="Heuristic · NeuroPlanner"/><meta property="twitter:title" content="Heuristic · NeuroPlanner"/><meta name="description" content="Documentation for NeuroPlanner."/><meta property="og:description" content="Documentation for NeuroPlanner."/><meta property="twitter:description" content="Documentation for NeuroPlanner."/><meta property="og:url" content="https://Pevnak.github.io/NeuroPlanner/heuristic/"/><meta property="twitter:url" content="https://Pevnak.github.io/NeuroPlanner/heuristic/"/><link rel="canonical" href="https://Pevnak.github.io/NeuroPlanner/heuristic/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/onlinestats.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">NeuroPlanner</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Heuristic</a><ul class="internal"><li><a class="tocitem" href="#Heuristic-functions"><span>Heuristic functions</span></a></li><li><a class="tocitem" href="#Remarks"><span>Remarks</span></a></li></ul></li><li><a class="tocitem" href="../usage_guide/">Usage guide</a></li><li><a class="tocitem" href="../theory/">Theoretical background</a></li><li><a class="tocitem" href="../losses/">Losses</a></li><li><a class="tocitem" href="../extractors/">Extractors</a></li><li><a class="tocitem" href="../model_representation/">Model representation</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Heuristic</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Heuristic</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/Pevnak/NeuroPlanner.jl/blob/main/docs/src/heuristic.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h2 id="Heuristic-functions"><a class="docs-heading-anchor" href="#Heuristic-functions">Heuristic functions</a><a id="Heuristic-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Heuristic-functions" title="Permalink"></a></h2><p>The heuristic function is composed of two parts. First, called <strong>extractor</strong>, <code>ex</code> takes a STRIPS state <code>s</code> and project it to to (computation) graph (more on this later). The second is the neural network <code>nn</code>, which takes the computation graph and project it to the heuristic function. This functionality is based on a custom extension of <a href="https://github.com/CTUAvastLab/Mill.jl">Mill.jl</a> library.  The complete heuristic function is therefore composition <code>nn ∘ ex</code>. </p><p>The advantage is that non-differentiable operations are kept in the the extractor part and differentiable in the neural network. The main advantage of the extractor producing the computational graph is that it can be then deduplicated, which speeds-up the traingn.</p><h3 id="Extractors"><a class="docs-heading-anchor" href="#Extractors">Extractors</a><a id="Extractors-1"></a><a class="docs-heading-anchor-permalink" href="#Extractors" title="Permalink"></a></h3><p>The library implements various methods how to represent STRIPS states as graphs. This representation is important for the properties of the heuristic function, mainly to its ability to discriminate between states. This representation is perpendicular to the type of graph neural networks, in which this library in not that interested that much. The functionality projecting state to graph is called <strong>extractor</strong>.</p><p>The available extractors are:</p><ul><li><code>ObjectAtom</code><sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> represent state as a hyper-multi-graph. Each vertex corresponds to the object, each atom represent a hyper-edge. Different types of atoms are represented as different type of edges.</li><li><code>ObjectAtomBip</code><sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> represent state as a multi-graph or graphs with features on edges. Each object and atom corresponds to a vertex. Object-vertex is connected to atom-vertex when object is an argument of the atom. The representation is similar to the <code>ObjectAtom</code>, except the hyper-edges are represented in bipartite graph.</li><li><code>ObjectBinary</code><sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> represent states as multi graph (or graph with features on edges). Each object correponds to the vertex. Vertices are connected by the edge if they are in the same atom. The type of edge (or features one edges) corresponds to the type of atom and position of the object in the argument.</li><li><code>AtomBinary</code><sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> represent states as multi graph (or graph with features on edges). Each object correponds to the atom. Vertices are connected by the edge if they share the same object. The type of the edge (or features one edges) corresponds to the position of the object in both atoms.</li><li><code>ObjectPair</code><sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> each vertex corresponds to a tuple of objects and edges are create by some cryptic algorithm.</li><li><code>ASNet</code><sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> creates vertices for each possible atoms. The atoms are present in the graph even when they are not <code>true</code> in the state. This means that graph representing states differ only in features on edges, which codes if the atom is <code>true</code> or <code>false.</code> </li><li><code>HGNN</code><sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup> is similar to <code>ASNet</code>, except the message-passing over the hyper-edges is a bit different, as it includes more domain knowledge from the planning community.</li></ul><p>Let&#39;s now focus on extraction function <code>ex</code>, which takes a state <code>s</code> and converts it to some representation suitable for NN. In the case of this library, the representation is an instance of <code>KnowledgeBase</code>, which encodes the copmutation graph. Since the extractor produces the compuation graph, the extraction function controls the number of graph convolutions and the presence of residual connections. </p><p>The extraction function is implemented as a callable struct with a following api, where <code>ObjectBinary</code> is used as an example: The api / interface of the extraction function is as follows:</p><ul><li><code>ex = ObjectBinary(domain; message_passes = 2, residual=:linear, edgebuilder = FeaturedEdgeBuilder)</code> –- initialize the extractor for a given domain. At this moment, we need to specify the number of message passes and the type of residula layer (<code>:none</code> or <code>:linear</code>). Additionally, you specify how to represent edges pf different types by passing the <code>edgebuilder</code>. The default  <code>FeaturedEdgeBuilder</code>, uses is edges with features, other option is <code>MultiEdgeBuilder</code>  which uses multi-graph (multiple)</li><li><code>ex = specialize(ex, problem)</code>  –- specialize the extractor functions for a given domain</li><li><code>ex = add_goalstate(ex, problem, goal = goalstate(domain, problem)</code> –- fixes a goal state in the extractor</li><li><code>ex = add_initstate(ex, problem, start = initstate(domain, problem)</code> –- fixes an initial state in the extractor.</li></ul><p>With this, <code>ex(state)</code> converts the state to a structure for the neural network, an instance of <code>KnowledgeBase</code> in this concerete example.</p><p>All extraction functions has to be initialized for a given domain and specialized for a given problem. This is typically needed to initiate various mapping to ensure it does not change between problem instances (an example is a map of categorical variables to one-hot representations).  Adding goal or init state is optional. If they are added, the input to the neural network would always contain <em>goal</em> or <em>init</em> state, in which case the neural network will measure a distance to a state. If they are not used, the neural network can be used to create and embedding of states. </p><h3 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h3><p>Load the libraries, domain, and problem</p><pre><code class="language-julia hljs">using NeuroPlanner
using NeuroPlanner.PDDL
using NeuroPlanner.Flux
using NeuroPlanner.Mill
using NeuroPlanner.SymbolicPlanners
using PlanningDomains

domain = load_domain(IPCInstancesRepo,&quot;ipc-2014&quot;, &quot;barman-sequential-satisficing&quot;)
problems = list_problems(IPCInstancesRepo, &quot;ipc-2014&quot;, &quot;barman-sequential-satisficing&quot;)
problem = load_problem(IPCInstancesRepo, &quot;ipc-2014&quot;, &quot;barman-sequential-satisficing&quot;, first(problems))</code></pre><p>First, we create the <code>ObjectBinary</code> for the <code>domain</code></p><pre><code class="language-julia hljs">julia&gt; ex = ObjectBinary(domain)
Unspecialized extractor for barman (6, 9)</code></pre><p>Then, we specialize the extractor for a problem</p><pre><code class="nohighlight hljs">julia&gt; specex = specialize(ex, problem)
Specialized extractor without goal for barman (6, 9, 40)</code></pre><p>If we would like to use the extractor to measure a distance to the goal, we add the goal</p><pre><code class="language-julia hljs">julia&gt; specex = add_goalstate(specex, problem,)
Specialized extractor with goal for barman (6, 9, 40)</code></pre><p>The function <code>add_goalstate</code> has a third implicit parameter <code>goal =  goalstate(domain, problem),</code> which allows to specify different goal then the default for the problem. Also, the function checks, if the extraction function is specialized for the problem and if not, it specialize it. Hence, the above can be shorted as </p><pre><code class="language-julia hljs">julia&gt; specex = add_goalstate(ObjectAtom(domain), problem)
Specialized extractor with goal for barman (6, 9, 40)</code></pre><p>With specialized extraction function, we can convert a state to a <code>KnowledgeBase</code> as </p><pre><code class="language-julia hljs">julia&gt; s = initstate(domain, problem);
julia&gt; specex(s)
KnowledgeBase: (x1,gnn_2,res_3,gnn_4,res_5,o)</code></pre><p>The neural network processing the <code>KnowledgeBase</code> can be initialized as the neural network in <a href="https://github.com/CTUAvastLab/Mill.jl">Mill.jl</a> library through extended <code>reflectinmodel</code> function</p><pre><code class="language-julia hljs">julia&gt; kb = specex(s);
julia&gt; model = reflectinmodel(specex(s), d -&gt; Dense(d,10), SegmentedMean;fsm = Dict(&quot;&quot; =&gt;  d -&gt; Dense(d,1)))
KnowledgeModel: (gnn_2,res_3,gnn_4,res_5,o)</code></pre><p>which finishes the construction of the heuristic function as </p><pre><code class="language-julia hljs">julia&gt; model(specex(s))
1×1 Matrix{Float32}:
 0.003934524</code></pre><p>The parameters of the model can be optimized using the standard method of <strong>Flux.jl</strong> on top of which they are built. We refer the reader to the documentation of <a href="https://github.com/CTUAvastLab/Mill.jl">Mill.jl</a> for details of the <code>reflectinmodel</code> function.</p><p>A complete example look like:</p><pre><code class="language-julia hljs">using NeuroPlanner
using NeuroPlanner.PDDL
using NeuroPlanner.Flux
using NeuroPlanner.Mill
using NeuroPlanner.SymbolicPlanners
using PlanningDomains

domain = load_domain(IPCInstancesRepo,&quot;ipc-2014&quot;, &quot;barman-sequential-satisficing&quot;)
problems = list_problems(IPCInstancesRepo, &quot;ipc-2014&quot;, &quot;barman-sequential-satisficing&quot;)
problem = load_problem(IPCInstancesRepo, &quot;ipc-2014&quot;, &quot;barman-sequential-satisficing&quot;, first(problems))

ex = ObjectAtom(domain)
specex = specialize(ex, problem)
specex = add_goalstate(ex, problem, goalstate(domain, problem))
s = initstate(domain, problem)
model = reflectinmodel(specex(s), d -&gt; Dense(d,10), SegmentedMean;fsm = Dict(&quot;&quot; =&gt;  d -&gt; Dense(d,1)))

model(specex(s))</code></pre><h2 id="Remarks"><a class="docs-heading-anchor" href="#Remarks">Remarks</a><a id="Remarks-1"></a><a class="docs-heading-anchor-permalink" href="#Remarks" title="Permalink"></a></h2><h3 id="First-remark:-the-model-is-general"><a class="docs-heading-anchor" href="#First-remark:-the-model-is-general">First remark: the model is general</a><a id="First-remark:-the-model-is-general-1"></a><a class="docs-heading-anchor-permalink" href="#First-remark:-the-model-is-general" title="Permalink"></a></h3><p>The model is able to process any problem instance despite it has been constructed from a state on a given problem instance. This can be seen on the following example which assumes the above model and uses the model on all problem instances from <em>barman</em>. Notice that the extractor needs to be specialized for every problem instance.</p><pre><code class="language-julia hljs">ex = ObjectAtom(domain)

map(problems) do problem_name
	problem = load_problem(IPCInstancesRepo, &quot;ipc-2014&quot;, &quot;barman-sequential-satisficing&quot;, problem_name)
	s = initstate(domain, problem)
	specex = add_goalstate(ex, problem)
	only(model(specex(s)))
end</code></pre><h3 id="Second-remark:-fixing-initial-state-measures-distance-from-initial-state"><a class="docs-heading-anchor" href="#Second-remark:-fixing-initial-state-measures-distance-from-initial-state">Second remark: fixing initial state measures distance from initial state</a><a id="Second-remark:-fixing-initial-state-measures-distance-from-initial-state-1"></a><a class="docs-heading-anchor-permalink" href="#Second-remark:-fixing-initial-state-measures-distance-from-initial-state" title="Permalink"></a></h3><p>If the extractor is specialized with the goalstate, it meaures a distances from the state to a goalstate. On the other hand if the extractor is specialized with the init state, it will measure distance from init state to a state. Hence a distance from init to goal state can be computed by both specializations as is shown in the following example</p><pre><code class="language-julia hljs">ex = ObjectAtom(domain)
problem = load_problem(IPCInstancesRepo, &quot;ipc-2014&quot;, &quot;barman-sequential-satisficing&quot;, first(problems))

iex = add_initstate(ex, problem)
gex = add_goalstate(ex, problem)

si = initstate(domain, problem)
gi = goalstate(domain, problem)
model = reflectinmodel(iex(si), d -&gt; Dense(d,10), SegmentedMean;fsm = Dict(&quot;&quot; =&gt;  d -&gt; Dense(d,1)))

model(iex(goalstate(domain, problem))) ≈ model(gex(initstate(domain, problem)))</code></pre><h3 id="Third-remark:-extractor-without-goal-is-useful-for-creating-an-embedding"><a class="docs-heading-anchor" href="#Third-remark:-extractor-without-goal-is-useful-for-creating-an-embedding">Third remark: extractor without goal is useful for creating an embedding</a><a id="Third-remark:-extractor-without-goal-is-useful-for-creating-an-embedding-1"></a><a class="docs-heading-anchor-permalink" href="#Third-remark:-extractor-without-goal-is-useful-for-creating-an-embedding" title="Permalink"></a></h3><p>May-be, we do not want a neural network to implement a heuristic function, but to project the state to a vector. This can be done with a specialized extractor without goal as </p><pre><code class="language-julia hljs">problem = load_problem(IPCInstancesRepo, &quot;ipc-2014&quot;, &quot;barman-sequential-satisficing&quot;, first(problems))
ex = specialize(ObjectAtom(domain), problem)

si = initstate(domain, problem)
gi = goalstate(domain, problem)
model = reflectinmodel(ex(si), d -&gt; Dense(d,10), SegmentedMean;fsm = Dict(&quot;&quot; =&gt;  d -&gt; Dense(d,3)))</code></pre><p>now the model will project states to the <code>3</code>-dimensional vector as</p><pre><code class="language-julia hljs">julia&gt; model(ex(si))
3×1 Matrix{Float32}:
  0.048694983
 -0.35071477
 -0.013481511
</code></pre><p>Notice the difference in the argument <code>fsm = Dict(&quot;&quot; =&gt;  d -&gt; Dense(d,3))</code> in the argument of the <code>reflectinmodel</code>.</p><h3 id="Fourth-remark:-extracted-states-can-be-batched"><a class="docs-heading-anchor" href="#Fourth-remark:-extracted-states-can-be-batched">Fourth remark: extracted states can be batched</a><a id="Fourth-remark:-extracted-states-can-be-batched-1"></a><a class="docs-heading-anchor-permalink" href="#Fourth-remark:-extracted-states-can-be-batched" title="Permalink"></a></h3><p><code>KnowledgeBase</code> supports <code>Flux.batch</code> for minibatching. Using the above example, we can create a minibatch containing initial and goal state as</p><pre><code class="language-julia hljs">julia&gt; b = Flux.batch([ex(si), ex(gi)])
KnowledgeBase: (x1,gnn_2,res_3,gnn_4,res_5,o)</code></pre><p>and project it with the model</p><pre><code class="language-julia hljs">julia&gt; model(b)
3×2 Matrix{Float32}:
  0.048695    0.061797
 -0.350715    0.111813
 -0.0134815  -0.0315447</code></pre><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Horčík, Rostislav, and Gustav Šír. &quot;Expressiveness of Graph Neural Networks in Planning Domains.&quot; Proceedings of the International Conference on Automated Planning and Scheduling. Vol. 34. 2024.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>Toyer, Sam, et al. &quot;Action schema networks: Generalised policies with deep learning.&quot; Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018.</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>Learning Domain-Independent Planning Heuristics with Hypergraph Networks, William Shen, Felipe Trevizan, Sylvie Thiebaux, 2020</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../usage_guide/">Usage guide »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Thursday 19 December 2024 08:11">Thursday 19 December 2024</span>. Using Julia version 1.10.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
